# Inner Product, Norms, Angles, Similarity & Orthogonality

---

## Inner Product and Angle

Two vectors are similar if their vectors point in the same direction and cosine measures that.

Cosine tells how much two vectors point in the same direction.

### Formula

$$
\cos \theta
=
\frac{\langle x, y \rangle}{\|x\| \, \|y\|}
$$

Here:

- ⟨x, y⟩ is the inner product  
- ‖x‖ and ‖y‖ are the magnitudes (norms)

---

## What Cosine Checks (Direction)

- \(\cos \theta = 1\) → same direction → maximum inner product  
- \(\cos \theta = 0\) → perpendicular → inner product = 0  
- \(\cos \theta = -1\) → opposite direction → negative inner product  

So cosine checks direction only.

---

## Direction vs Magnitude

- Direction tells which features move together  
- Magnitude tells how strong the vector is  

Direction tells pattern  
Magnitude tells strength

---

## Cosine and Similarity Meaning

- \(\cos \theta = 1\) → high similarity  
- \(\cos \theta = 0\) → unrelated data  
- \(\cos \theta = -1\) → opposite behavior  

So cosine directly tells similarity of behavior.

---

## Inner Product (Conceptual Definition)

Inner product is an operation defined between two vectors in a vector space that returns a real number which measures how strongly they are aligned.

---

## Inner Product Measures Strength of Similarity

- Inner product → measures how strong similarity is  
- Norm → comes from inner product  

---

## How Magnitude (Norm) Comes from Inner Product

Take a vector with itself:

$$
\langle x, x \rangle
=
\|x\| \, \|x\| \cos 0
=
\|x\|^2
$$

Since \(\cos 0 = 1\),

$$
\|x\| = \sqrt{\langle x, x \rangle}
$$

---

## Steps to Compute Norm

- Take vector \(x\)  
- Take inner product with itself  
- Take square root  
- Result is its length (norm)  

This length is called norm generated by inner product.

---

## Generalized Norm

A norm created using inner product is called a generalized norm.

$$
\|x\| := \sqrt{\langle x, x \rangle}
$$

---

## Cauchy–Schwarz Inequality  
### (FIRST Definition – Intuitive)

Similarity between two vectors can NEVER be larger than the product of their lengths.

This means:

- You cannot be “more aligned” than your sizes allow.

---

## Cauchy–Schwarz Inequality  
### (SECOND Definition – Formal)

The dot product (inner product) of two vectors can NEVER be larger than the product of their generalized norms (lengths).

Mathematically:

$$
|\langle x, y \rangle|
\le
\|x\| \, \|y\|
$$

⚠️ This is not optional — this inequality is the key.

---

## Why Cauchy–Schwarz Is Important

- It guarantees the norm generated by inner product is valid  
- It ensures all norm axioms are satisfied  
- Without it, inner-product-based norm would fail  

---

## Conclusion from Cauchy–Schwarz

Inner product defines similarity  
and using Cauchy–Schwarz  
it also generates a valid notion of length (norm)

---

## Inner Product with Itself (Euclidean Meaning)

Inner product with itself means:

- Square each component  
- Sum them  
- Then take square root  

This gives Euclidean norm.

---

## Important Property

Every inner product space is automatically a normed space  
because inner product generates a norm.

---

## Polarization Identity

**Question:**  
If I have a generated norm, can I recover the inner product?

Yes.

$$
\langle x, y \rangle
=
\frac{1}{2}
\left(
\|x+y\|^2
-
\|x\|^2
-
\|y\|^2
\right)
$$

**Intuition:**

- Measure how long combined vector is  
- Remove individual parts  
- Remaining term is interaction (inner product)  

---

## Key Result of Polarization Identity

In inner product spaces,  
inner product is completely determined by the norm.

---

## Orthogonality

Two vectors are orthogonal if:

$$
\langle x, y \rangle = 0
$$

### Meaning

- No shared direction  
- No shared information  
- Independent features  

---

## Properties of Orthogonal Vectors

- One vector adds nothing about the other  
- Changing one does not affect the other  
- No interference  

---

## Orthogonality in Machine Learning

- If vectors interfere → learning is inefficient and confusing  
- If vectors are orthogonal → learning is:
  - clean  
  - stable  
  - interpretable  

More orthogonality → more new information

---

## One-Sentence Takeaway (Your Line)

In inner product space, two vectors are orthogonal exactly when the squared length of their sum equals the sum of their squared lengths.

$$
\|x+y\|^2
=
\|x\|^2
+
\|y\|^2
$$

---

## How Similarity Is Found

Two vectors are similar if:

- coordinates are close overall  
- angle between them is small  

$$
\text{Similarity}(x,y)
=
\frac{x \cdot y}{\|x\| \, \|y\|}
$$

This combines all coordinate contributions.

---

## Final Core Meaning

- Inner product → alignment  
- Norm → strength  
- Cosine → direction similarity  
- Orthogonality → independent information  
